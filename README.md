# ğŸ¥ Medical Insurance Premium Prediction

Final project for **BZAN3307 (Machine Learning & Artificial Intelligence)**  

ğŸ‘‰ Dataset: [Medical Insurance Premium Prediction â€” Kaggle](https://www.kaggle.com/datasets/tejashvi14/medical-insurance-premium-prediction/data)  

---

## ğŸ“Œ Project Overview
Health insurance premiums have risen significantly in recent years, creating financial uncertainty for families.  
This project develops machine learning models to predict **yearly medical insurance premiums** based on customer demographics and health features, using a simulated dataset of ~1,000 records.  

**Dataset features (10 predictors):**  
- Quantitative: Age, Height, Weight, Number of Major Surgeries  
- Binary: Diabetes, Blood Pressure Problems, Any Transplants, Any Chronic Diseases, Known Allergies, Family Cancer History  
- **Target:** `PremiumPrice` (annual insurance cost, in INR â‚¹)  

---

## ğŸ”¬ Methodology
1. **Data Profiling & EDA**
   - No missing values or sparsity issues  
   - Age ~ Uniform (18â€“66, mean 42)  
   - Height ~ Normal (145â€“188 cm, mean 168)  
   - Weight ~ Right-skewed (51â€“132 kg, mean 77)  
   - Surgeries: 0â€“3, mostly 0â€“1  
   - Premiums ranged â‚¹15kâ€“â‚¹40k, mean â‚¹24,336, slightly right-skewed  

2. **Model Training & Evaluation**
   Several regression models were trained, tuned, and compared:  
   - **Linear Regression**: train RÂ² = 0.65, test RÂ² = 0.61  
   - **Neural Network Regressor**: reduced overfitting, test RÂ² â‰ˆ 0.65â€“0.69 (still unstable on small data)  
   - **Support Vector Regression (SVR)**: very poor fit, RÂ² near 0  
   - **Random Forest Regressor**: best performance â€” train RÂ² = 0.89, test RÂ² = 0.87  
     - Fine-tuning (`min_samples_split=10`) reduced overfitting  

3. **Feature Importance (Random Forest)**
   - Age (70%)  
   - Weight (~13%)  
   - Blood Pressure Problems (~11%)  

4. **Outlier & Transformation Tests**
   - Removing outliers (df_trim) did not improve Random Forest performance and risked excluding under/overweight cases.  
   - Transformations of `PremiumPrice`:  
     - **Log**: improved overall RÂ² to 0.88 but still biased toward low premiums  
     - **Box-Cox (Î»=0.75)**: balanced bias, overall RÂ² = 0.85, better at predicting higher premiums  
     - **Quantile transform**: highest test RÂ² (0.90), but extremely biased toward low premiums  

---

## ğŸ“ˆ Key Findings
- **Random Forest Regressor** (with Box-Cox transformed PremiumPrice) was the most **ethical** and balanced model, despite not having the absolute highest RÂ².  
- Feature importance shifted after Box-Cox transformation:  
  - **Chronic Diseases** rose in importance (~10%) â†’ valuable for predicting higher costs.  
  - **Weight** dropped in importance (from ~13% â†’ ~4%).  
  - **Blood Pressure Problems** declined to ~1%.  
- Models tended to **overpredict low premiums** and **underpredict high premiums**, highlighting dataset bias.  
- More representative sampling (stratifying premiums into low/medium/high) would improve fairness.  

---

## âš–ï¸ Ethical Considerations
- Prioritizing the most accurate model (quantile transform, RÂ² = 0.90) would worsen bias against predicting high premiums accurately.  
- The **Box-Cox Random Forest model** (RÂ² = 0.85) was chosen as the most ethical because it improved prediction for high-cost individuals, where accurate forecasting is most critical.  

---

## ğŸ› ï¸ Tech Stack
- **Python**  
  - Pandas, NumPy (data preprocessing)  
  - Matplotlib, Seaborn (EDA & visualization)  
  - Scikit-learn (Linear Regression, SVR, Random Forest, Neural Network)  
- **Jupyter Notebook** for workflow  

---

## ğŸ“š Acknowledgments
- **BZAN3307 â€” Machine Learning & Artificial Intelligence** course final project  
- Dataset by [Tejashvi on Kaggle](https://www.kaggle.com/datasets/tejashvi14/medical-insurance-premium-prediction/data) (CC0 license)  
- Report and analysis by Julie Hohenberg  

---
